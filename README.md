# CLIP Implementation from Scratch

Welcome to the CLIP implementation repository! This project aims to build a **Contrastive Language-Image Pretraining (CLIP)** model from scratch, with the future goal of extending the implementation to include and test various advanced CLIP variants.

## Project Overview

CLIP (Contrastive Language-Image Pretraining) is a model designed by OpenAI that learns to associate images and text in a meaningful way. This repository is dedicated to implementing CLIP from the ground up, focusing on understanding the core components of the model. As the project progresses, we plan to explore and test different advanced variants of CLIP to enhance its capabilities and performance.

## Motivation

One of the key motivations behind this project is to test and explore **compositionality** in vision-language models. By building CLIP from scratch and experimenting with various advanced variants, we aim to better understand how these models can be improved to handle complex, compositional tasks where the relationship between vision and language is more intricate.

## Future Plans
- **Implement Advanced CLIP Variants**: Once the base CLIP model is complete, we'll experiment with advanced versions, such as SLIP, FLIP, and others.
- **Testing and Benchmarking**: We will develop a suite of tests to evaluate the performance and accuracy of each CLIP variant.
- **Exploration of New Architectures**: As the project evolves, we plan to explore modifications to the architecture that could improve the model's performance on specific tasks.
